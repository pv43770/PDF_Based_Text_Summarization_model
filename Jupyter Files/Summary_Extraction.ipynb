{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff7b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7dbdae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "True\n",
      "12.4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.version.cuda)  # To check if CUDA is available in PyTorch\n",
    "print(torch.cuda.device_count())  # Should show the number of available GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708e9b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|█████████████████████████████████████████████████████████████████| 18/18 [01:49<00:00,  6.08s/it]\n",
      "C:\\Users\\pv437\\anaconda3\\envs\\python11\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\pv437\\anaconda3\\envs\\python11\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:497: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [0/9], Loss: 7.1358\n",
      "Epoch [1/3] completed. Average Loss: 3.4120\n",
      "Epoch [2/3], Step [0/9], Loss: 0.6857\n",
      "Epoch [2/3] completed. Average Loss: 0.7172\n",
      "Epoch [3/3], Step [0/9], Loss: 0.3059\n",
      "Epoch [3/3] completed. Average Loss: 0.2640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_summary_tokenizer\\\\tokenizer_config.json',\n",
       " './fine_tuned_summary_tokenizer\\\\special_tokens_map.json',\n",
       " './fine_tuned_summary_tokenizer\\\\vocab.json',\n",
       " './fine_tuned_summary_tokenizer\\\\merges.txt',\n",
       " './fine_tuned_summary_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, AdamW\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "# Set up spaCy for text preprocessing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "    return extracted_text\n",
    "\n",
    "# Function to preprocess text (remove stop words, lowercase)\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return ' '.join([token.text for token in doc if not token.is_stop])\n",
    "\n",
    "# Custom Dataset class for PDFs\n",
    "class PDFDataset(Dataset):\n",
    "    def __init__(self, pdf_folder):\n",
    "        self.pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        self.data = []\n",
    "        \n",
    "        for file_path in tqdm(self.pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            if not text.strip():\n",
    "                print(f\"Warning: {file_path} is empty or contains no extractable text.\")\n",
    "                continue\n",
    "            processed_text = preprocess_text(text)\n",
    "            summary = ' '.join(processed_text.split()[:50])\n",
    "            self.data.append((processed_text, summary))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, summary = self.data[idx]\n",
    "        inputs = tokenizer(text, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        targets = tokenizer(summary, max_length=150, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}  # Remove batch dimension\n",
    "        targets = targets['input_ids'].squeeze(0)\n",
    "        return inputs, targets\n",
    "\n",
    "# Path to folder containing PDFs\n",
    "folder_path = r\"C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\"\n",
    "\n",
    "# Load dataset and create DataLoader\n",
    "dataset = PDFDataset(folder_path)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Training settings\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Move inputs and labels to device (GPU/CPU)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save model using PyTorch\n",
    "torch.save(model.state_dict(), \"./fine_tuned_summary_model.pth\")\n",
    "\n",
    "# Save tokenizer using Huggingface's native method\n",
    "tokenizer.save_pretrained(\"./fine_tuned_summary_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f9fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pv437\\anaconda3\\envs\\python11\\Lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pv437\\anaconda3\\envs\\python11\\Lib\\site-packages\\transformers\\generation\\utils.py:1338: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for pdf1.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf10.pdf...\n",
      "Summary for pdf10.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf11.pdf...\n",
      "Summary for pdf11.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf12.pdf...\n",
      "Summary for pdf12.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf13.pdf...\n",
      "Summary for pdf13.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf14.pdf...\n",
      "Summary for pdf14.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf15.pdf...\n",
      "Summary for pdf15.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf16.pdf...\n",
      "Applying structured text filter for pdf16.pdf...\n",
      "Summary for pdf16.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf17.pdf...\n",
      "Applying structured text filter for pdf17.pdf...\n",
      "Summary for pdf17.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf18.pdf...\n",
      "Summary for pdf18.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf2.pdf...\n",
      "Summary for pdf2.pdf has been stored in MongoDB.\n",
      "Processing C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\\pdf3.pdf...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pymongo\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import pdfplumber\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['pdf_summaries']  # Database name\n",
    "collection = db['summaries']  # Collection name\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = './fine_tuned_summary_model'\n",
    "tokenizer_path = './fine_tuned_summary_tokenizer'\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Function to extract text from a PDF file and get metadata\n",
    "def extract_text_and_metadata_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or ''  # Handle None return value\n",
    "        # Get PDF metadata: file size and number of pages\n",
    "        pdf_info = {\n",
    "            'name': os.path.basename(pdf_path),\n",
    "            'size': os.path.getsize(pdf_path),  # File size in bytes\n",
    "            'pages': len(pdf.pages)\n",
    "        }\n",
    "    return text, pdf_info\n",
    "\n",
    "# Function to detect and remove structured text like logs, dates, and numbers\n",
    "def filter_structured_text(text):\n",
    "    # Regular expression to match patterns like dates, times, transaction IDs, etc.\n",
    "    pattern = r'(\\d{1,2}-\\d{1,2}-\\d{2,4}|\\d{1,2}:\\d{2}:\\d{2}|\\d{1,2}/\\d{1,2}/\\d{2,4}|\\d{3,}|[Bb]uy|[Ss]ell|price|log|rate|order id|qty|trd id)'\n",
    "    filtered_text = re.sub(pattern, '', text)  # Remove matches of the pattern\n",
    "    return filtered_text.strip()  # Strip leading/trailing whitespace\n",
    "\n",
    "# Function to chunk large text into smaller pieces\n",
    "def chunk_text(text, max_tokens=1024):\n",
    "    tokens = text.split()\n",
    "    return [' '.join(tokens[i:i + max_tokens]) for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "# Function to tokenize text for model input\n",
    "def tokenize_text(text, tokenizer):\n",
    "    return tokenizer(text, max_length=1024, return_tensors='pt', truncation=True).to(device)\n",
    "\n",
    "# Function to generate a summary for each chunk\n",
    "def generate_summary(inputs, model, tokenizer):\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=150, \n",
    "        min_length=30, \n",
    "        length_penalty=1.5, \n",
    "        num_beams=5, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Summarize a single PDF and store summary in MongoDB\n",
    "def summarize_and_store_pdf(pdf_path, model, tokenizer):\n",
    "    text, pdf_info = extract_text_and_metadata_from_pdf(pdf_path)\n",
    "    \n",
    "    # Check if there's enough text to summarize\n",
    "    if not text.strip():\n",
    "        return \"No text extracted for summary.\"\n",
    "    \n",
    "    # Apply filtering only for pdf16 and pdf17\n",
    "    pdf_name = pdf_info['name'].lower()\n",
    "    if \"pdf16\" in pdf_name or \"pdf17\" in pdf_name:\n",
    "        print(f\"Applying structured text filter for {pdf_name}...\")\n",
    "        text = filter_structured_text(text)\n",
    "    \n",
    "    # Split the text into chunks if it's too large\n",
    "    chunks = chunk_text(text, max_tokens=500)\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenize_text(chunk, tokenizer)\n",
    "        summary = generate_summary(inputs, model, tokenizer)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    # Combine all chunk summaries\n",
    "    final_summary = ' '.join(summaries)\n",
    "    \n",
    "    # Add the summary and PDF metadata to MongoDB\n",
    "    pdf_summary_data = {\n",
    "        'pdf_name': pdf_info['name'],\n",
    "        'pdf_size_bytes': pdf_info['size'],\n",
    "        'pdf_pages': pdf_info['pages'],\n",
    "        'summary': final_summary\n",
    "    }\n",
    "    collection.insert_one(pdf_summary_data)\n",
    "    \n",
    "    print(f\"Summary for {pdf_info['name']} has been stored in MongoDB.\")\n",
    "\n",
    "# Summarize all PDFs in a folder and store them in MongoDB\n",
    "def summarize_all_pdfs_in_folder_and_store(folder_path, model, tokenizer):\n",
    "    pdf_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            summarize_and_store_pdf(pdf_file, model, tokenizer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "\n",
    "# Folder containing the PDFs\n",
    "folder_path = r\"C:\\Users\\pv437\\Desktop\\Data Scince Folder\\Pankaj Assignments\\Wasserstoff\\Deployment\\Flask\\Downloaded_pdfs\"\n",
    "\n",
    "# Summarize all PDFs in the folder and store the results in MongoDB\n",
    "summarize_all_pdfs_in_folder_and_store(folder_path, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ebad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient  # Import MongoClient\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['pdf_summaries']  # Database name\n",
    "collection = db['summaries']  # Collection name\n",
    "\n",
    "\n",
    "# Function to get PDF summary by name\n",
    "def get_pdf_summary(pdf_name):\n",
    "    # Query the database for the specified PDF name\n",
    "    pdf_record = collection.find_one({'pdf_name': pdf_name})\n",
    "    \n",
    "    if pdf_record:\n",
    "        print(f\"Summary for {pdf_record['pdf_name']}:\")\n",
    "        print(f\"File Size: {pdf_record['pdf_size_bytes']} bytes\")\n",
    "        print(f\"Number of Pages: {pdf_record['pdf_pages']}\")\n",
    "        print(f\"Summary: {pdf_record['summary']}\")\n",
    "    else:\n",
    "        print(f\"No summary found for the PDF named '{pdf_name}'.\")\n",
    "\n",
    "# Get user input for the PDF name\n",
    "user_input_pdf_name = input(\"Enter the PDF name (including .pdf extension): \")\n",
    "get_pdf_summary(user_input_pdf_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78294c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f9d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
